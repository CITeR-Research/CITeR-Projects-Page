<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="A webpage containing all the information about the CITeR Projects">
  <meta name="keywords" content="CITeR, citer, CITER">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Center for Identification Technology Research - CITeR</title>


  <!-- ✅ Add your custom table header styles here -->
  <style>
    table thead th {
      text-align: center;
      vertical-align: middle;
      background-color: #f3f4f6;
      font-weight: 600;
      font-size: 1rem;
      padding: 12px 10px;
      border-bottom: 2px solid #d1d5db;
      color: #222;
    }

    table thead tr:hover {
      background-color: #e5e7eb;
      transition: background-color 0.2s ease-in-out;
    }

    table td {
      padding: 10px 12px;
      vertical-align: top;
    }
  </style>






  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://keunhong.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            Useful Links
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://citer.clarkson.edu/">
              CITeR Home
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/affiliates/">
              Affiliates
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/research-resources/">
              Research & Resources
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/affiliates/affiliate-login/">
              Affiliate Login
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/become-an-affiliate/">  
              Become an Affiliate
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Center for Identification Technology Research - CITeR</h1>
            <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span>
          </div> -->

            <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div> -->

            <!-- <div class="column has-text-centered"> -->
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span> -->
              </a>
            </div>

          </div>
        </div>
      </div>
    </div>
    </div>
  </section>





  <section class="section">
    <div class="container" style="width:95%; margin:0 auto;">
      <h2 class="title is-3 has-text-centered">CITeR Code & Dataset Repositories</h2>

      <div class="table-container" style="overflow-x:auto;">
        <table class="table is-fullwidth is-striped is-hoverable is-bordered" style="margin:auto;">
       <colgroup>
          <col style="width: 10%;">  <!-- Project ID -->
          <col style="width: 20%;">  <!-- Project Title -->
          <col style="width: 20%;">  <!-- PI and Researchers -->
          <col style="width: 10%;">  <!-- Repository Link -->
          <col style="width: 45%;">  <!-- Description -->
        </colgroup>


          <thead>
            <tr>
              <th class="has-text-centered" >Project ID</th>
              <th class="has-text-centered">Project Title</th>
              <th class="has-text-centered" >PI and Researchers</th>
              <th class="has-text-centered">Repository Link</th>
              <th class="has-text-centered">Description</th>
            </tr>
          </thead>
          <tbody>
            <!-- First Row of Projects -->
            <tr>
              <td>1650503</td>
              <td>A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-world Noisy
                Environments</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Md Jahangir Alam Khondkar</li>
                  <li style="white-space:nowrap;">Ajan Ahmed</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz</li>
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:mkhondka@charlotte.edu">mkhondka@charlotte.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered" >
                <a href="https://github.com/jahangirkhondkar/DL_SpeechEnhancementToolkit" target="_blank"
                  rel="noopener">
                  GitHub Code
                  <br>
                  (Public)
                </a>
              </td>
              <td>The "DL_SpeechEnhancementToolkit" is a collection of deep learning models—including Wave‑U‑Net, CMGAN,
                and UNet for enhancing speech quality under noisy real-world conditions. It provides end-to-end code
                for data preprocessing, model training, evaluation (including SNR, PESQ, and speaker-recognition
                metrics).</td>
            </tr>

            <!-- Second Row of Projects  -->
            <tr>
              <td>#22S-01M</td>
              <td>On the Capacity and Uniqueness of Synthetic Face Images</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Vishnu Boddeti</li>
                  <li style="white-space:nowrap;">Arun Ross</li>
                  <li style="white-space:nowrap;">Email: <a href="vishnu@msu.edu">vishnu@msu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/human-analysis/capacity-generative-face-models" target="_blank"
                  rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                Code to reproduce all the results of the publication resulting from this project.
              </td>
            </tr>

            <!-- Third Row of Projects  -->
            <tr>
              <td>#22-01J-SP</td>
              <td>Fully Homomorphic Encryption in Biometrics</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Vishnu Boddeti</li>
                  <li style="white-space:nowrap;">Arun Ross</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:vishnu@msu.edu">vishnu@msu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/human-analysis/encrypted-biometric-fusion" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                Code to reproduce the results of the paper published as part of this project.
              </td>
            </tr>

            <!-- Fourth Row of Projects  -->
            <tr>
              <td>#24S-04W</td>
              <td>Deep-Learning-Based Generation of Synthetic Contactless Fingerphotos</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Jeremy Dawson</li>
                  <li style="white-space:nowrap;">Christopher Burton</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:chb0012@mix.wvu.edu">chb0012@mix.wvu.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/ChrisBurton2/SyntheticContactlessFingerprintGenerator" target="_blank"
                  rel="noopener">
                  GitHub Code <br>
                  (Private)
                </a>
              </td>
              <td>
                This project explores the synthetic generation of contactless fingerphotos using two deep learning
                approaches:
                image translation and sample synthesis. The repository contains Python-based implementations of CycleGAN
                and a
                Diffusion model, each with instructions for separate execution. CycleGAN was trained on a paired dataset
                of contact
                and contactless fingerprint images, demonstrating image translation capabilities. The Diffusion model
                focuses on
                synthetic generation, trained on fingerphotos collected with modern mobile devices.
              </td>
            </tr>



            <tr>
              <td>#23S-05W</td>
              <td>Performance Evaluation of Cross-Spectral Iris Matching: Visible vs NIR</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Jeremy Dawson</li>
                  <li style="white-space:nowrap;">Hannah Anderson</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:hba0002@mix.wvu.edu">hba0002@mix.wvu.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/hba0002/Cross-Spectral-Iris-VIS-NIR.git" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                The goal of this work is to develop a method for iris translation between the NIR and VIS spectra using
                Generative
                Adversarial Networks (GANs). Image enhancement techniques are applied in the cropped and normalized iris
                domains,
                and a classifier component is incorporated into the GAN to preserve identity information during image
                translation.
                The system simultaneously synthesizes translations in both directions (NIR→VIS and VIS→NIR) during
                training.
              </td>
            </tr>


            <tr>
              <td>#23S-08CB</td>
              <td>Performance Benchmark: Ear-only vs. Ear+Face Fusion Biometrics for Adult and Children</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers (CU)</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz (CU)</li>
                  <li style="white-space:nowrap;">Dr. Nalini Ratha (UB)</li>
                  <li style="white-space:nowrap;">Afzal Hossain (CU)</li>
                  <li style="white-space:nowrap;">Dr. Mohammad Zahir Uddin Chowdhury (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:mochowd@clarkson.edu">mochowd@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/mdzahirdu/ear-detection-yolov5-custom" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Private)
                </a>
              </td>
              <td>
                This repository implements a custom-trained YOLOv5 pipeline for detecting and cropping ear regions from
                profile face images.
                Using 100 manually annotated samples from the Clarkson Child Dataset, the YOLOv5 model was fine-tuned to
                accurately localize
                ears in side-view facial imagery. The <code>ear_detection.py</code> script loads the trained
                <code>model.pt</code> and processes each input
                image to generate high-quality cropped ear images for downstream analysis. It supports batch inference,
                configurable paths,
                and confidence-based bounding box selection. The tool is designed for biometric research, medical
                preprocessing, or any task
                requiring precise ear localization. Full setup instructions and dependencies are included for easy
                integration and reproducibility.
              </td>
            </tr>


            <tr>
              <td>#23S-08CB</td>
              <td>Performance Benchmark: Ear-only vs. Ear+Face Fusion Biometrics for Adult and Children</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers (CU)</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz (CU)</li>
                  <li style="white-space:nowrap;">Dr. Nalini Ratha (UB)</li>
                  <li style="white-space:nowrap;">Afzal Hossain (CU)</li>
                  <li style="white-space:nowrap;">Dr. Mohammad Zahir Uddin Chowdhury (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:mochowd@clarkson.edu">mochowd@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/mdzahirdu/ear-normalization-pipeline" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>

              <td>
                This repository provides a complete pipeline for ear normalization and flattening based on landmark
                detection and
                geometric alignment. Starting with ear crops detected using a custom YOLOv5 model, the pipeline
                localizes key
                landmarks using a pretrained TensorFlow <code>.pb</code> model (Hansley et al., 2018). It then applies
                geometric transformations
                to normalize ear orientation and shape across samples. Finally, PCA-based flattening is performed to
                generate 2D
                representations suitable for biometric analysis. The pipeline supports both single-image
                (<code>demo.py</code>) and batch
                processing (<code>main.py</code>) modes, making it adaptable for research and deployment. This
                implementation closely
                follows methods described in the literature for ear alignment and template generation.
              </td>
            </tr>


            <tr>
              <td>#24S</td>
              <td>Biometric Aging in Children — Phase V</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Mst Rumana Sumi</li>
                  <li style="white-space:nowrap;">Laura Holsopple</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz</li>
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:sumima@clarkson.edu">sumima@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/RumanaSum/Customized-Preciozzi-Fingerprint-Scaling" target="_blank"
                  rel="noopener">
                  GitHub Code <br>
                  (Private)
                </a>
              </td>
              <td>
                This GitHub repository provides a MATLAB-based pipeline for age-adaptive fingerprint scaling. A
                regression script fits
                polynomial models to Preciozzi’s published scale factors (ages 1, 2, 5–9 yrs) and adulthood, then
                predicts missing
                values for ages 3, 4, and 10–17 based on the best-fitting model. A rescaling function applies each
                subject’s
                age-specific scale factor via bicubic interpolation to normalize ridge spacing to an adult reference (~9
                px @ 500 ppi).
                Finally, a batch script reads enrollment ages from Excel, processes all images in a directory tree, and
                outputs scaled
                prints in a parallel folder structure—ensuring compatibility with commercial fingerprint matchers.
              </td>
            </tr>


            <tr>
              <td>#24F-02C</td>
              <td>Development of Extended-Length Audio Dataset for Advanced Deepfake Synthesis and Detection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz (CU)</li>
                  <li style="white-space:nowrap;">Rahul Vijaykumar (CU)</li>
                  <li style="white-space:nowrap;">Ajan Ahmed (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:mimtiaz@clarkson.edu">mimtiaz@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/AVHBAC/ELAD-SVDSR" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>

              <td>
                Participant count: 36 speakers (each ~45 minutes of read-aloud natural speech in controlled sessions).
                Recording setup:
                five high-quality microphones per session (parallel channels). Synthetic speech: matched synthetic
                voices for 20 subjects
                using a mix of open-source and commercial systems (e.g., Tortoise TTS, ElevenLabs), enabling paired
                natural ↔ synthetic
                comparisons. Format: WAV audio files (original sample rates preserved). Metadata & privacy: demographics
                and
                capture-condition metadata provided; personally identifying information is anonymized. Access & hosting:
                public release
                via IEEE DataPort (Standard Dataset; access typically requires an IEEE DataPort subscription).
              </td>
            </tr>


            <tr>
              <td>#23F-02W</td>
              <td>Improving Performance of Facial Recognition via Multi-Model Algorithmic Ensemble</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Prashnna Gyawali</li>
                  <li style="white-space:nowrap;">Nima Najafzadeh</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:prashnna.gyawali@mail.wvu.edu">prashnna.gyawali@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/nimanjfz/Ensemble-Knowledge-Distillation" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>

              <td>
                This repository contains code for an ensemble knowledge distillation project, where knowledge from
                multiple teacher
                models is distilled into a single student network to improve the overall performance of facial
                recognition systems.
              </td>
            </tr>


            <tr>
              <td>#24S-01W</td>
              <td>A Facial Image Quality Toolbox Based on ISO/IEC 29794-5 Specification</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Prashnna Gyawali</li>
                  <li style="white-space:nowrap;">Nima Najafzadeh</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:prashnna.gyawali@mail.wvu.edu">prashnna.gyawali@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/nimanjfz/Face-Image-Quality" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                This repository contains code for training a model that produces a unified score to assess face image
                quality,
                integrating multiple quality factors into a single metric. The implementation aligns with the ISO/IEC
                29794-5
                specification for biometric image quality evaluation and supports extensibility for research and
                benchmarking
                across different datasets.
              </td>
            </tr>


            <tr>
              <td>#24S-04B</td>
              <td>Investigating Molecular Fingerprints of Human Tissue using Multi-spectral Photoacoustic Imaging</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Jun Xia (UB)</li>
                  <li style="white-space:nowrap;">Dr. Srirangaraj Setlur (UB)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:junxia@buffalo.edu">junxia@buffalo.edu</a></li>
                </ul>
              </td>
              <td>
                <span style="color:#777;">No code for this project.</span>
              </td>
              <td>
                This project involves a preliminary investigation into the use of multi-spectral photoacoustic imaging
                to identify
                molecular fingerprints of human tissue. The study explores feasibility and imaging methodology, but no
                software or
                code artifacts are currently associated with this project.
              </td>
            </tr>















            <!-- Add more project rows below -->
          </tbody>
        </table>
      </div>
    </div>
  </section>


<!-- Video Display Section -->
<section class="section" style="display:flex; flex-direction:column; align-items:center; padding:40px 0;">
  <h2 class="title is-3 has-text-centered" style="margin-bottom:24px;">CITeR Activities</h2>

  <div style="
      width:1280px;
      height:720px;
      overflow:hidden;
      border-radius:12px;
      box-shadow:0 0 12px rgba(0,0,0,0.25);
      position:relative;
      background:#000;
    ">
    <video id="original-frame-video"
           autoplay
           muted
           loop
           playsinline
           preload="metadata"
           style="
             position:absolute;
             top:50%;
             left:50%;
             transform:translate(-50%, -50%);
             min-width:100%;
             min-height:100%;
             object-fit:cover;
           ">
      <source src="https://citer.clarkson.edu/wp-content/uploads/2019/08/Biometrics.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </div>
</section>





<section class="section" style="background-color:#fafafa; padding:40px 0;">
  <h2 class="title is-3 has-text-centered" style="margin-bottom:24px;">Affiliates</h2>
  <div class="container" style="max-width:1500px; margin:0 auto; text-align:center;">
    <div style="
      display:flex;
      justify-content:space-around;
      align-items:center;
      flex-wrap:wrap;
      gap:30px;
    ">
      <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/stacked-grn-logo.png" alt="Clarkson University" style="height:200px; object-fit:contain;">
      <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/west-virginia.png" alt="West Virginia University" style="height:200px; object-fit:contain;">
      <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/buffalo.png" alt="University at Buffalo" style="height:200px; object-fit:contain;">
      <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/michigan.png" alt="Michigan State University" style="height:200px; object-fit:contain;">
      <img src="https://citer.clarkson.edu/wp-content/uploads/2020/01/idiap-logo.png" alt="Idiap Research Institute" style="height:200px; object-fit:contain;">
      <img src="https://citer.clarkson.edu/wp-content/uploads/2025/04/8716-01-Charlotte-Master-File-v7_1-1-1024x1024.png" alt="UNC Charlotte" style="height:200px; object-fit:contain;">
    </div>
  </div>
</section>






  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/redwankarimsony" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>