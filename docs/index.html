<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="A webpage containing all the information about ongoing CITeR Projects">
  <meta name="keywords" content="CITeR, citer, CITER">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Center for Identification Technology Research - CITeR</title>


  <!-- ✅ Add your custom table header styles here -->
  <style>
    table thead th {
      text-align: center;
      vertical-align: middle;
      background-color: #f3f4f6;
      font-weight: 600;
      font-size: 1rem;
      padding: 12px 10px;
      border-bottom: 2px solid #d1d5db;
      color: #222;
    }

    table thead tr:hover {
      background-color: #e5e7eb;
      transition: background-color 0.2s ease-in-out;
    }

    table td {
      padding: 10px 12px;
      vertical-align: top;
    }
  </style>






  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon-citer.jpg" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://keunhong.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            Useful Links
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://citer.clarkson.edu/">
              CITeR Home
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/affiliates/">
              Affiliates
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/research-resources/">
              Research & Resources
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/affiliates/affiliate-login/">
              Affiliate Login
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/become-an-affiliate/">
              Become an Affiliate
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Center for Identification Technology Research - CITeR</h1>
            <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span>
          </div> -->

            <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div> -->

            <!-- <div class="column has-text-centered"> -->
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://citer.clarkson.edu/" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Home</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/CITeR-Research" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub Page</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span> -->
              </a>
            </div>

          </div>
        </div>
      </div>
    </div>
    </div>
  </section>





  <section class="section">
    <div class="container" style="width:95%; margin:0 auto;">
      <h2 class="title is-3 has-text-centered">CITeR Code Repositories</h2>

      <div class="table-container" style="overflow-x:auto;">
        <table class="table is-fullwidth is-striped is-hoverable is-bordered" style="margin:auto;">
          <colgroup>
            <col style="width: 10%;"> <!-- Project ID -->
            <col style="width: 20%;"> <!-- Project Title -->
            <col style="width: 20%;"> <!-- PI and Researchers -->
            <col style="width: 10%;"> <!-- Repository Link -->
            <col style="width: 45%;"> <!-- Description -->
          </colgroup>


          <thead>
            <tr>
              <th class="has-text-centered">Project ID</th>
              <th class="has-text-centered">Project Title</th>
              <th class="has-text-centered">PI and Researchers</th>
              <th class="has-text-centered">Repository Link</th>
              <th class="has-text-centered">Description</th>
            </tr>
          </thead>
          <tbody>
            <!-- First Row of Projects -->
            <tr>
              <td class="has-text-centered">1650503</td>
              <td>A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-world Noisy
                Environments</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Md Jahangir Alam Khondkar</li>
                  <li style="white-space:nowrap;">Ajan Ahmed</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz</li>
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:mkhondka@charlotte.edu">mkhondka@charlotte.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/jahangirkhondkar/DL_SpeechEnhancementToolkit" target="_blank"
                  rel="noopener">
                  GitHub Code
                  <br>
                  (Public)
                </a>
              </td>
              <td>The "DL_SpeechEnhancementToolkit" is a collection of deep learning models—including Wave‑U‑Net, CMGAN,
                and UNet for enhancing speech quality under noisy real-world conditions. It provides end-to-end code
                for data preprocessing, model training, evaluation (including SNR, PESQ, and speaker-recognition
                metrics).</td>
            </tr>

            <!-- Second Row of Projects  -->
            <tr>
              <td class="has-text-centered">#22S-01M</td>
              <td>On the Capacity and Uniqueness of Synthetic Face Images</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Vishnu Boddeti</li>
                  <li style="white-space:nowrap;">Arun Ross</li>
                  <li style="white-space:nowrap;">Email: <a href="vishnu@msu.edu">vishnu@msu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/human-analysis/capacity-generative-face-models" target="_blank"
                  rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                Code to reproduce all the results of the publication resulting from this project.
              </td>
            </tr>

            <!-- Third Row of Projects  -->
            <tr>
              <td class="has-text-centered">#22-01J-SP</td>
              <td>Fully Homomorphic Encryption in Biometrics</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Vishnu Boddeti</li>
                  <li style="white-space:nowrap;">Arun Ross</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:vishnu@msu.edu">vishnu@msu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/human-analysis/encrypted-biometric-fusion" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                Code to reproduce the results of the paper published as part of this project.
              </td>
            </tr>

            <!-- Fourth Row of Projects  -->
            <tr>
              <td class="has-text-centered">#24S-04W</td>
              <td>Deep-Learning-Based Generation of Synthetic Contactless Fingerphotos</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Jeremy Dawson</li>
                  <li style="white-space:nowrap;">Christopher Burton</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:chb0012@mix.wvu.edu">chb0012@mix.wvu.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/ChrisBurton2/SyntheticContactlessFingerprintGenerator" target="_blank"
                  rel="noopener">
                  GitHub Code <br>
                  (Private)
                </a>
              </td>
              <td>
                This project explores the synthetic generation of contactless fingerphotos using two deep learning
                approaches:
                image translation and sample synthesis. The repository contains Python-based implementations of CycleGAN
                and a
                Diffusion model, each with instructions for separate execution. CycleGAN was trained on a paired dataset
                of contact
                and contactless fingerprint images, demonstrating image translation capabilities. The Diffusion model
                focuses on
                synthetic generation, trained on fingerphotos collected with modern mobile devices.
              </td>
            </tr>



            <tr>
              <td class="has-text-centered">#23S-05W</td>
              <td>Performance Evaluation of Cross-Spectral Iris Matching: Visible vs NIR</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Jeremy Dawson</li>
                  <li style="white-space:nowrap;">Hannah Anderson</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:hba0002@mix.wvu.edu">hba0002@mix.wvu.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/hba0002/Cross-Spectral-Iris-VIS-NIR.git" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                The goal of this work is to develop a method for iris translation between the NIR and VIS spectra using
                Generative
                Adversarial Networks (GANs). Image enhancement techniques are applied in the cropped and normalized iris
                domains,
                and a classifier component is incorporated into the GAN to preserve identity information during image
                translation.
                The system simultaneously synthesizes translations in both directions (NIR→VIS and VIS→NIR) during
                training.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#23S-08CB</td>
              <td>Performance Benchmark: Ear-only vs. Ear+Face Fusion Biometrics for Adult and Children</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers (CU)</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz (CU)</li>
                  <li style="white-space:nowrap;">Dr. Nalini Ratha (UB)</li>
                  <li style="white-space:nowrap;">Afzal Hossain (CU)</li>
                  <li style="white-space:nowrap;">Dr. Mohammad Zahir Uddin Chowdhury (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:mochowd@clarkson.edu">mochowd@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/mdzahirdu/ear-detection-yolov5-custom" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Private)
                </a>
              </td>
              <td>
                This repository implements a custom-trained YOLOv5 pipeline for detecting and cropping ear regions from
                profile face images.
                Using 100 manually annotated samples from the Clarkson Child Dataset, the YOLOv5 model was fine-tuned to
                accurately localize
                ears in side-view facial imagery. The <code>ear_detection.py</code> script loads the trained
                <code>model.pt</code> and processes each input
                image to generate high-quality cropped ear images for downstream analysis. It supports batch inference,
                configurable paths,
                and confidence-based bounding box selection. The tool is designed for biometric research, medical
                preprocessing, or any task
                requiring precise ear localization. Full setup instructions and dependencies are included for easy
                integration and reproducibility.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#23S-08CB</td>
              <td>Performance Benchmark: Ear-only vs. Ear+Face Fusion Biometrics for Adult and Children</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers (CU)</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz (CU)</li>
                  <li style="white-space:nowrap;">Dr. Nalini Ratha (UB)</li>
                  <li style="white-space:nowrap;">Afzal Hossain (CU)</li>
                  <li style="white-space:nowrap;">Dr. Mohammad Zahir Uddin Chowdhury (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:mochowd@clarkson.edu">mochowd@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/mdzahirdu/ear-normalization-pipeline" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>

              <td>
                This repository provides a complete pipeline for ear normalization and flattening based on landmark
                detection and
                geometric alignment. Starting with ear crops detected using a custom YOLOv5 model, the pipeline
                localizes key
                landmarks using a pretrained TensorFlow <code>.pb</code> model (Hansley et al., 2018). It then applies
                geometric transformations
                to normalize ear orientation and shape across samples. Finally, PCA-based flattening is performed to
                generate 2D
                representations suitable for biometric analysis. The pipeline supports both single-image
                (<code>demo.py</code>) and batch
                processing (<code>main.py</code>) modes, making it adaptable for research and deployment. This
                implementation closely
                follows methods described in the literature for ear alignment and template generation.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#24S</td>
              <td>Biometric Aging in Children — Phase V</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Mst Rumana Sumi</li>
                  <li style="white-space:nowrap;">Laura Holsopple</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz</li>
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:sumima@clarkson.edu">sumima@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/RumanaSum/Customized-Preciozzi-Fingerprint-Scaling" target="_blank"
                  rel="noopener">
                  GitHub Code <br>
                  (Private)
                </a>
              </td>
              <td>
                This GitHub repository provides a MATLAB-based pipeline for age-adaptive fingerprint scaling. A
                regression script fits
                polynomial models to Preciozzi’s published scale factors (ages 1, 2, 5–9 yrs) and adulthood, then
                predicts missing
                values for ages 3, 4, and 10–17 based on the best-fitting model. A rescaling function applies each
                subject’s
                age-specific scale factor via bicubic interpolation to normalize ridge spacing to an adult reference (~9
                px @ 500 ppi).
                Finally, a batch script reads enrollment ages from Excel, processes all images in a directory tree, and
                outputs scaled
                prints in a parallel folder structure—ensuring compatibility with commercial fingerprint matchers.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">#23F-02W</td>
              <td>Improving Performance of Facial Recognition via Multi-Model Algorithmic Ensemble</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Prashnna Gyawali</li>
                  <li style="white-space:nowrap;">Nima Najafzadeh</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:prashnna.gyawali@mail.wvu.edu">prashnna.gyawali@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/nimanjfz/Ensemble-Knowledge-Distillation" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>

              <td>
                This repository contains code for an ensemble knowledge distillation project, where knowledge from
                multiple teacher
                models is distilled into a single student network to improve the overall performance of facial
                recognition systems.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#24S-01W</td>
              <td>A Facial Image Quality Toolbox Based on ISO/IEC 29794-5 Specification</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Prashnna Gyawali</li>
                  <li style="white-space:nowrap;">Nima Najafzadeh</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:prashnna.gyawali@mail.wvu.edu">prashnna.gyawali@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/nimanjfz/Face-Image-Quality" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                This repository contains code for training a model that produces a unified score to assess face image
                quality,
                integrating multiple quality factors into a single metric. The implementation aligns with the ISO/IEC
                29794-5
                specification for biometric image quality evaluation and supports extensibility for research and
                benchmarking
                across different datasets.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#24S-04B</td>
              <td>Investigating Molecular Fingerprints of Human Tissue using Multi-spectral Photoacoustic Imaging</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Jun Xia (UB)</li>
                  <li style="white-space:nowrap;">Dr. Srirangaraj Setlur (UB)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:junxia@buffalo.edu">junxia@buffalo.edu</a></li>
                </ul>
              </td>
              <td>
                <span style="color:#777;">No code for this project.</span>
              </td>
              <td>
                This project involves a preliminary investigation into the use of multi-spectral photoacoustic imaging
                to identify
                molecular fingerprints of human tissue. The study explores feasibility and imaging methodology, but no
                software or
                code artifacts are currently associated with this project.
              </td>
            </tr>
            <!-- Add more project rows below -->
          </tbody>
        </table>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container" style="width:95%; margin:0 auto;">
      <h2 class="title is-3 has-text-centered">CITeR Dataset Repositories</h2>

      <div class="table-container" style="overflow-x:auto;">
        <table class="table is-fullwidth is-striped is-hoverable is-bordered" style="margin:auto;">
          <colgroup>
            <col style="width: 10%;"> <!-- Project ID -->
            <col style="width: 20%;"> <!-- Project Title -->
            <col style="width: 20%;"> <!-- PI and Researchers -->
            <col style="width: 10%;"> <!-- Repository Link -->
            <col style="width: 45%;"> <!-- Description -->
          </colgroup>


          <thead>
            <tr>
              <th class="has-text-centered">Project ID</th>
              <th class="has-text-centered">Project Title</th>
              <th class="has-text-centered">PI and Researchers</th>
              <th class="has-text-centered">Repository Link</th>
              <th class="has-text-centered">Description</th>
            </tr>
          </thead>

          <tbody>
            <!-- First Row of Projects -->
            <tr>
              <td class="has-text-centered">#24F-02C</td>
              <td>Development of Extended-Length Audio Dataset for Advanced Deepfake Synthesis and Detection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz (CU)</li>
                  <li style="white-space:nowrap;">Rahul Vijaykumar (CU)</li>
                  <li style="white-space:nowrap;">Ajan Ahmed (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:mimtiaz@clarkson.edu">mimtiaz@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/AVHBAC/ELAD-SVDSR" target="_blank" rel="noopener">
                  GitHub<br>
                  Page
                </a>
              </td>

              <td>
                Participant count: 36 speakers (each ~45 minutes of read-aloud natural speech in controlled sessions).
                Recording setup: five high-quality microphones per session (parallel channels). Synthetic speech:
                matched synthetic
                voices for 20 subjects using a mix of open-source and commercial systems (e.g., Tortoise TTS,
                ElevenLabs), enabling paired
                natural ↔ synthetic comparisons. Format: WAV audio files (original sample rates preserved). Metadata &
                privacy: demographics
                and capture-condition metadata provided; personally identifying information is anonymized. Access &
                hosting:
                public release via IEEE DataPort (Standard Dataset; access typically requires an IEEE DataPort
                subscription).
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">#####</td>
              <td>Clarkson University :Multi-modal Longitudinal Children Biometric Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Multi-modal-Longitudinal-Children-Biometric">GitHub
                  <br> Page</a>
              </td>
              <td>
                This longitudinal biometric dataset was collected from six different modalities at approximately
                six-month intervals over a span of 9.5 years at Potsdam Elementary and Potsdam High School. The
                collection, funded by the
                Center for Identification Technology Research (CITeR) and the National Science Foundation, includes:
                Fingerprints,
                Footprints, Face, Ear, NIR Iris, and Voice Recordings. The dataset aims to support research on biometric
                aging in children and
                enhance biometric recognition technologies for younger populations.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">#####</td>
              <td>Clarkson University: Multimodal Longitudinal Infant–Toddler Biometric Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Multi-modal-Longitudinal-Children-Biometric">GitHub
                  <br> Page</a>
              </td>
              <td>
                This longitudinal biometric dataset was collected from six different modalities at approximately 15-day
                to 3-month
                intervals over a span of two years at the Canton Pediatric Office and Clarkson University. The
                collection, funded by
                the Center for Identification Technology Research (CITeR) and the National Science Foundation, includes:
                Fingerprints, Footprints, Face, Ear, NIR Iris, and Voice Recordings. The dataset supports research in
                early-age
                biometric development and recognition performance in infants and toddlers.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">#####</td>
              <td>IBM-UB Online and Offline Multilingual Handwriting Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Srirangaraj Setlur (UB)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:setlur@buffalo.edu">setlur@buffalo.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Clarkson-University-Multimodal-Longitudinal-Infant-Toddler-Biometric-Dataset"
                  target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The IBM-UB dataset is a bilingual and bi-modal handwriting corpus containing both
                <strong>online</strong> (stylus-captured)
                and <strong>offline</strong> (scanned) handwritten documents in multiple languages. It includes
                free-form text, structured forms,
                isolated words, and symbols. This dataset supports multilingual OCR and information retrieval research.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>UB RidgeBase Fingerprint Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">University at Buffalo (UB)</li>
                  <li style="white-space:nowrap;">Contact: <a
                      href="cubs-ridgebase@buffalo.edu">cubs-ridgebase@buffalo.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a class="has-text-centered"
                  href="https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html" target="_blank"
                  rel="noopener">
                  RidgeBase <br> Dataset <br> Page
                </a>

              </td>
              <td>
                RidgeBase is a large-scale benchmark dataset for advancing research in <strong>contactless fingerprint
                  recognition</strong>
                using smartphone cameras. It contains over 15,000 contactless and contact-based fingerprint image pairs
                collected from
                88 individuals under diverse conditions. The dataset supports evaluation of multiple matching scenarios:
                <ul style="margin-top:5px; margin-bottom:5px; padding-left:20px;">
                  <li>Task 1: Single Finger Matching</li>
                  <li>Task 2: Four Finger Matching</li>
                  <li>Task 3: Set-Based Matching</li>
                </ul>
                RidgeBase enables benchmarking for both contactless-to-contactless (CL2CL) and contact-to-contactless
                (C2CL)
                verification and identification research.
              </td>

            <tr>
              <td class="has-text-centered">#######</td>
              <td>GestSpoof: Gesture-Based Spatio-Temporal Representation Learning for Robust Fingerprint Presentation
                Attack Detection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">University at Buffalo (UB)</li>
                  <li style="white-space:nowrap;">Contact: <a
                      href="mailto:cubs-gestspoof@buffalo.edu">cubs-gestspoof@buffalo.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://www.buffalo.edu/cubs/research/datasets/gestspoof-dataset.html" target="_blank"
                  rel="noopener">
                  Dataset <br> Page
                </a>
              </td>
              <td>
                The GestSpoof dataset contains both <strong>real</strong> and <strong>spoof</strong> fingerprint samples
                collected from approximately <strong>23 subjects</strong> using three spoof materials
                (<em>bodydouble_alja</em>, <em>ecoflex_alja</em>, and <em>gelatin_bodydouble</em>). Fingers include
                index, middle, ring, and little, each captured under five motion gestures (<em>Vertical</em>,
                <em>Horizontal</em>, <em>Diag1</em>, <em>Diag2</em>, and <em>Twist</em>). This dataset enables
                spatio-temporal modeling for motion-based fingerprint presentation attack detection by capturing
                differences in elastic deformation between authentic and synthetic fingerprints.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>GestSpoof: Gesture-Based Spatio-Temporal Representation Learning for Robust Fingerprint Presentation
                Attack Detection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">University at Buffalo (UB)</li>
                  <li style="white-space:nowrap;">Contact: <a
                      href="mailto:cubs-gestspoof@buffalo.edu">cubs-gestspoof@buffalo.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://www.buffalo.edu/cubs/research/datasets/gestspoof-dataset.html" target="_blank"
                  rel="noopener">
                  Dataset <br> Page
                </a>
              </td>
              <td>
                The GestSpoof dataset contains both <strong>real</strong> and <strong>spoof</strong> fingerprint samples
                collected from approximately <strong>23 subjects</strong> using three spoof materials
                (<em>bodydouble_alja</em>, <em>ecoflex_alja</em>, and <em>gelatin_bodydouble</em>). Fingers include
                index, middle, ring, and little, each captured under five motion gestures (<em>Vertical</em>,
                <em>Horizontal</em>, <em>Diag1</em>, <em>Diag2</em>, and <em>Twist</em>). This dataset enables
                spatio-temporal modeling for motion-based fingerprint presentation attack detection by capturing
                differences in elastic deformation between authentic and synthetic fingerprints.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Multimodal Biometric Dataset Collection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:WVUBiometricData@mail.wvu.edu">WVUBiometricData@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Multimodal-Biometric-Collection" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The Multimodal Biometric Dataset Collection is a multisite database developed by <strong>West Virginia
                  University</strong> and <strong>Clarkson University</strong> for research under the <strong>CITeR
                  Database Release Agreement</strong>. It includes <strong>iris, face, face video, voice, fingerprint,
                  hand geometry</strong>, and <strong>palmprint</strong> data for over 500 subjects collected across
                multiple visits. The dataset supports multimodal biometric fusion and longitudinal analysis, and
                includes releases: <em>WVU Release 1</em>, <em>WVU Release 2</em>, and <em>CU Release 2</em>.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Quality—Face/Iris Research Ensemble (Q-FIRE)</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Funded by: DHS S&amp;T Directorate and NSF</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Quality-Face-Iris-Research-Ensemble" target="_blank"
                  rel="noopener">
                  GitHub <br>Page
                </a>
              </td>
              <td>
                The Q-FIRE dataset, developed at <strong>Clarkson University</strong>, contains <strong>iris</strong>
                and <strong>face</strong> image sequences collected at varying distances and quality levels. It was
                funded by the <strong>Department of Homeland Security (DHS) Science and Technology Directorate</strong>
                in cooperation with the <strong>National Science Foundation (NSF)</strong>. The dataset supports
                research on multi-biometric fusion and image quality assessment, and was designed for use in <a
                  href="http://iris.nist.gov/irexII/" target="_blank" rel="noopener">IREX II</a> evaluations.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Keystroke Dataset Collection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Keystroke-Collection" target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The Keystroke Dataset was collected at <strong>Clarkson University</strong> from <strong>39
                  participants</strong> across two laboratory sessions, each containing approximately 10K keystrokes.
                The dataset includes <strong>password entries</strong>, <strong>free text responses</strong>, and
                <strong>transcription tasks</strong>, with synchronized video recordings of participants’ facial
                expressions and hand movements. It supports research in <strong>behavioral biometrics</strong> and
                <strong>keystroke dynamics</strong> under controlled experimental settings.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Facial Makeup Datasets</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Michigan State University (MSU)</li>
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Datasets-Facial-Makeup" target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The Facial Makeup Datasets comprise four collections designed to study the effects of <strong>cosmetic
                  makeup</strong> on face recognition performance: <em>YMU</em> (YouTube Makeup), <em>VMU</em> (Virtual
                Makeup), <em>MIW</em> (Makeup in the Wild), and <em>MIFS</em> (Makeup Induced Face Spoofing). These
                datasets include both real and synthetically modified female face images sourced from online media and
                controlled repositories, supporting research in <strong>makeup-robust face recognition</strong> and
                <strong>presentation attack detection</strong>.
              </td>
            </tr>






          </tbody>
        </table>
      </div>
    </div>
  </section>





  <!-- Video Display Section -->
  <section class="section" style="display:flex; flex-direction:column; align-items:center; padding:40px 0;">
    <h2 class="title is-3 has-text-centered" style="margin-bottom:24px;">CITeR Activities</h2>

    <div style="
      width:1280px;
      height:720px;
      overflow:hidden;
      border-radius:12px;
      box-shadow:0 0 12px rgba(0,0,0,0.25);
      position:relative;
      background:#000;
    ">
      <video id="original-frame-video" autoplay muted loop playsinline preload="metadata" style="
             position:absolute;
             top:50%;
             left:50%;
             transform:translate(-50%, -50%);
             min-width:100%;
             min-height:100%;
             object-fit:cover;
           ">
        <source src="https://citer.clarkson.edu/wp-content/uploads/2019/08/Biometrics.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
  </section>





  <section class="section" style="background-color:#fafafa; padding:40px 0;">
    <h2 class="title is-3 has-text-centered" style="margin-bottom:24px;">Affiliates</h2>
    <div class="container" style="max-width:1500px; margin:0 auto; text-align:center;">
      <div style="
      display:flex;
      justify-content:space-around;
      align-items:center;
      flex-wrap:wrap;
      gap:30px;
    ">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/stacked-grn-logo.png" alt="Clarkson University"
          style="height:200px; object-fit:contain;">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/west-virginia.png"
          alt="West Virginia University" style="height:200px; object-fit:contain;">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/buffalo.png" alt="University at Buffalo"
          style="height:200px; object-fit:contain;">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/michigan.png" alt="Michigan State University"
          style="height:200px; object-fit:contain;">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2020/01/idiap-logo.png" alt="Idiap Research Institute"
          style="height:200px; object-fit:contain;">
        <img
          src="https://citer.clarkson.edu/wp-content/uploads/2025/04/8716-01-Charlotte-Master-File-v7_1-1-1024x1024.png"
          alt="UNC Charlotte" style="height:200px; object-fit:contain;">
      </div>
    </div>
  </section>






  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/redwankarimsony" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>