<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="A webpage containing all the information about ongoing CITeR Projects">
  <meta name="keywords" content="CITeR, citer, CITER">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Center for Identification Technology Research - CITeR</title>


  <!-- ✅ Add your custom table header styles here -->
  <style>
    table thead th {
      text-align: center;
      vertical-align: middle;
      background-color: #f3f4f6;
      font-weight: 600;
      font-size: 1rem;
      padding: 12px 10px;
      border-bottom: 2px solid #d1d5db;
      color: #222;
    }

    table thead tr:hover {
      background-color: #e5e7eb;
      transition: background-color 0.2s ease-in-out;
    }

    table td {
      padding: 10px 12px;
      vertical-align: top;
    }
  </style>






  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon-citer.jpg" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://keunhong.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            Useful Links
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://citer.clarkson.edu/">
              CITeR Home
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/affiliates/">
              Affiliates
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/research-resources/">
              Research & Resources
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/affiliates/affiliate-login/">
              Affiliate Login
            </a>
            <a class="navbar-item" href="https://citer.clarkson.edu/become-an-affiliate/">
              Become an Affiliate
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Center for Identification Technology Research - CITeR</h1>
            <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span>
          </div> -->

            <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div> -->

            <!-- <div class="column has-text-centered"> -->
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://citer.clarkson.edu/" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Home</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/CITeR-Research" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub Page</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span> -->
              </a>
            </div>

          </div>
        </div>
      </div>
    </div>
    </div>
  </section>





  <section class="section">
    <div class="container" style="width:95%; margin:0 auto;">
      <h2 class="title is-3 has-text-centered">CITeR Code Repositories</h2>

      <div class="table-container" style="overflow-x:auto;">
        <table class="table is-fullwidth is-striped is-hoverable is-bordered" style="margin:auto;">
          <colgroup>
            <col style="width: 10%;"> <!-- Project ID -->
            <col style="width: 20%;"> <!-- Project Title -->
            <col style="width: 20%;"> <!-- PI and Researchers -->
            <col style="width: 10%;"> <!-- Repository Link -->
            <col style="width: 45%;"> <!-- Description -->
          </colgroup>


          <thead>
            <tr>
              <th class="has-text-centered">Project ID</th>
              <th class="has-text-centered">Project Title</th>
              <th class="has-text-centered">PI and Researchers</th>
              <th class="has-text-centered">Repository Link</th>
              <th class="has-text-centered">Description</th>
            </tr>
          </thead>
          <tbody>
            <!-- First Row of Projects -->
            <tr>
              <td class="has-text-centered">1650503</td>
              <td>A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-world Noisy
                Environments</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Md Jahangir Alam Khondkar</li>
                  <li style="white-space:nowrap;">Ajan Ahmed</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz</li>
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:mkhondka@charlotte.edu">mkhondka@charlotte.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/jahangirkhondkar/DL_SpeechEnhancementToolkit" target="_blank"
                  rel="noopener">
                  GitHub Code
                  <br>
                  (Public)
                </a>
              </td>
              <td>The "DL_SpeechEnhancementToolkit" is a collection of deep learning models—including Wave‑U‑Net, CMGAN,
                and UNet for enhancing speech quality under noisy real-world conditions. It provides end-to-end code
                for data preprocessing, model training, evaluation (including SNR, PESQ, and speaker-recognition
                metrics).</td>
            </tr>

            <!-- Second Row of Projects  -->
            <tr>
              <td class="has-text-centered">#22S-01M</td>
              <td>On the Capacity and Uniqueness of Synthetic Face Images</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Vishnu Boddeti</li>
                  <li style="white-space:nowrap;">Arun Ross</li>
                  <li style="white-space:nowrap;">Email: <a href="vishnu@msu.edu">vishnu@msu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/human-analysis/capacity-generative-face-models" target="_blank"
                  rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                Code to reproduce all the results of the publication resulting from this project.
              </td>
            </tr>

            <!-- Third Row of Projects  -->
            <tr>
              <td class="has-text-centered">#22-01J-SP</td>
              <td>Fully Homomorphic Encryption in Biometrics</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Vishnu Boddeti</li>
                  <li style="white-space:nowrap;">Arun Ross</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:vishnu@msu.edu">vishnu@msu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/human-analysis/encrypted-biometric-fusion" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                Code to reproduce the results of the paper published as part of this project.
              </td>
            </tr>

            <!-- Fourth Row of Projects  -->
            <tr>
              <td class="has-text-centered">#24S-04W</td>
              <td>Deep-Learning-Based Generation of Synthetic Contactless Fingerphotos</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Jeremy Dawson</li>
                  <li style="white-space:nowrap;">Christopher Burton</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:chb0012@mix.wvu.edu">chb0012@mix.wvu.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/ChrisBurton2/SyntheticContactlessFingerprintGenerator" target="_blank"
                  rel="noopener">
                  GitHub Code <br>
                  (Private)
                </a>
              </td>
              <td>
                This project explores the synthetic generation of contactless fingerphotos using two deep learning
                approaches:
                image translation and sample synthesis. The repository contains Python-based implementations of CycleGAN
                and a
                Diffusion model, each with instructions for separate execution. CycleGAN was trained on a paired dataset
                of contact
                and contactless fingerprint images, demonstrating image translation capabilities. The Diffusion model
                focuses on
                synthetic generation, trained on fingerphotos collected with modern mobile devices.
              </td>
            </tr>



            <tr>
              <td class="has-text-centered">#23S-05W</td>
              <td>Performance Evaluation of Cross-Spectral Iris Matching: Visible vs NIR</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Jeremy Dawson</li>
                  <li style="white-space:nowrap;">Hannah Anderson</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:hba0002@mix.wvu.edu">hba0002@mix.wvu.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/hba0002/Cross-Spectral-Iris-VIS-NIR.git" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                The goal of this work is to develop a method for iris translation between the NIR and VIS spectra using
                Generative
                Adversarial Networks (GANs). Image enhancement techniques are applied in the cropped and normalized iris
                domains,
                and a classifier component is incorporated into the GAN to preserve identity information during image
                translation.
                The system simultaneously synthesizes translations in both directions (NIR→VIS and VIS→NIR) during
                training.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#23S-08CB</td>
              <td>Performance Benchmark: Ear-only vs. Ear+Face Fusion Biometrics for Adult and Children</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers (CU)</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz (CU)</li>
                  <li style="white-space:nowrap;">Dr. Nalini Ratha (UB)</li>
                  <li style="white-space:nowrap;">Afzal Hossain (CU)</li>
                  <li style="white-space:nowrap;">Dr. Mohammad Zahir Uddin Chowdhury (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:mochowd@clarkson.edu">mochowd@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/mdzahirdu/ear-detection-yolov5-custom" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Private)
                </a>
              </td>
              <td>
                This repository implements a custom-trained YOLOv5 pipeline for detecting and cropping ear regions from
                profile face images.
                Using 100 manually annotated samples from the Clarkson Child Dataset, the YOLOv5 model was fine-tuned to
                accurately localize
                ears in side-view facial imagery. The <code>ear_detection.py</code> script loads the trained
                <code>model.pt</code> and processes each input
                image to generate high-quality cropped ear images for downstream analysis. It supports batch inference,
                configurable paths,
                and confidence-based bounding box selection. The tool is designed for biometric research, medical
                preprocessing, or any task
                requiring precise ear localization. Full setup instructions and dependencies are included for easy
                integration and reproducibility.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#23S-08CB</td>
              <td>Performance Benchmark: Ear-only vs. Ear+Face Fusion Biometrics for Adult and Children</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers (CU)</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz (CU)</li>
                  <li style="white-space:nowrap;">Dr. Nalini Ratha (UB)</li>
                  <li style="white-space:nowrap;">Afzal Hossain (CU)</li>
                  <li style="white-space:nowrap;">Dr. Mohammad Zahir Uddin Chowdhury (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:mochowd@clarkson.edu">mochowd@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/mdzahirdu/ear-normalization-pipeline" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>

              <td>
                This repository provides a complete pipeline for ear normalization and flattening based on landmark
                detection and
                geometric alignment. Starting with ear crops detected using a custom YOLOv5 model, the pipeline
                localizes key
                landmarks using a pretrained TensorFlow <code>.pb</code> model (Hansley et al., 2018). It then applies
                geometric transformations
                to normalize ear orientation and shape across samples. Finally, PCA-based flattening is performed to
                generate 2D
                representations suitable for biometric analysis. The pipeline supports both single-image
                (<code>demo.py</code>) and batch
                processing (<code>main.py</code>) modes, making it adaptable for research and deployment. This
                implementation closely
                follows methods described in the literature for ear alignment and template generation.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#24S</td>
              <td>Biometric Aging in Children — Phase V</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Mst Rumana Sumi</li>
                  <li style="white-space:nowrap;">Laura Holsopple</li>
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz</li>
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:sumima@clarkson.edu">sumima@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/RumanaSum/Customized-Preciozzi-Fingerprint-Scaling" target="_blank"
                  rel="noopener">
                  GitHub Code <br>
                  (Private)
                </a>
              </td>
              <td>
                This GitHub repository provides a MATLAB-based pipeline for age-adaptive fingerprint scaling. A
                regression script fits
                polynomial models to Preciozzi’s published scale factors (ages 1, 2, 5–9 yrs) and adulthood, then
                predicts missing
                values for ages 3, 4, and 10–17 based on the best-fitting model. A rescaling function applies each
                subject’s
                age-specific scale factor via bicubic interpolation to normalize ridge spacing to an adult reference (~9
                px @ 500 ppi).
                Finally, a batch script reads enrollment ages from Excel, processes all images in a directory tree, and
                outputs scaled
                prints in a parallel folder structure—ensuring compatibility with commercial fingerprint matchers.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">#23F-02W</td>
              <td>Improving Performance of Facial Recognition via Multi-Model Algorithmic Ensemble</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Prashnna Gyawali</li>
                  <li style="white-space:nowrap;">Nima Najafzadeh</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:prashnna.gyawali@mail.wvu.edu">prashnna.gyawali@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/nimanjfz/Ensemble-Knowledge-Distillation" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>

              <td>
                This repository contains code for an ensemble knowledge distillation project, where knowledge from
                multiple teacher
                models is distilled into a single student network to improve the overall performance of facial
                recognition systems.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#24S-01W</td>
              <td>A Facial Image Quality Toolbox Based on ISO/IEC 29794-5 Specification</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Prashnna Gyawali</li>
                  <li style="white-space:nowrap;">Nima Najafzadeh</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:prashnna.gyawali@mail.wvu.edu">prashnna.gyawali@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/nimanjfz/Face-Image-Quality" target="_blank" rel="noopener">
                  GitHub Code <br>
                  (Public)
                </a>
              </td>
              <td>
                This repository contains code for training a model that produces a unified score to assess face image
                quality,
                integrating multiple quality factors into a single metric. The implementation aligns with the ISO/IEC
                29794-5
                specification for biometric image quality evaluation and supports extensibility for research and
                benchmarking
                across different datasets.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#24S-04B</td>
              <td>Investigating Molecular Fingerprints of Human Tissue using Multi-spectral Photoacoustic Imaging</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Jun Xia (UB)</li>
                  <li style="white-space:nowrap;">Dr. Srirangaraj Setlur (UB)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:junxia@buffalo.edu">junxia@buffalo.edu</a></li>
                </ul>
              </td>
              <td>
                <span style="color:#777;">No code for this project.</span>
              </td>
              <td>
                This project involves a preliminary investigation into the use of multi-spectral photoacoustic imaging
                to identify
                molecular fingerprints of human tissue. The study explores feasibility and imaging methodology, but no
                software or
                code artifacts are currently associated with this project.
              </td>
            </tr>
            <!-- Add more project rows below -->
          </tbody>
        </table>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container" style="width:95%; margin:0 auto;">
      <h2 class="title is-3 has-text-centered">CITeR Dataset Repositories</h2>

      <div class="table-container" style="overflow-x:auto;">
        <table class="table is-fullwidth is-striped is-hoverable is-bordered" style="margin:auto;">
          <colgroup>
            <col style="width: 10%;"> <!-- Project ID -->
            <col style="width: 20%;"> <!-- Project Title -->
            <col style="width: 20%;"> <!-- PI and Researchers -->
            <col style="width: 10%;"> <!-- Repository Link -->
            <col style="width: 45%;"> <!-- Description -->
          </colgroup>


          <thead>
            <tr>
              <th class="has-text-centered">Project ID</th>
              <th class="has-text-centered">Project Title</th>
              <th class="has-text-centered">PI and Researchers</th>
              <th class="has-text-centered">Repository Link</th>
              <th class="has-text-centered">Description</th>
            </tr>
          </thead>

          <tbody>
            <!-- First Row of Projects -->
            <tr>
              <td class="has-text-centered">#24F-02C</td>
              <td>Development of Extended-Length Audio Dataset for Advanced Deepfake Synthesis and Detection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Masudul Haider Imtiaz (CU)</li>
                  <li style="white-space:nowrap;">Rahul Vijaykumar (CU)</li>
                  <li style="white-space:nowrap;">Ajan Ahmed (CU)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:mimtiaz@clarkson.edu">mimtiaz@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/AVHBAC/ELAD-SVDSR" target="_blank" rel="noopener">
                  GitHub<br>
                  Page
                </a>
              </td>

              <td>
                Participant count: 36 speakers (each ~45 minutes of read-aloud natural speech in controlled sessions).
                Recording setup: five high-quality microphones per session (parallel channels). Synthetic speech:
                matched synthetic
                voices for 20 subjects using a mix of open-source and commercial systems (e.g., Tortoise TTS,
                ElevenLabs), enabling paired
                natural ↔ synthetic comparisons. Format: WAV audio files (original sample rates preserved). Metadata &
                privacy: demographics
                and capture-condition metadata provided; personally identifying information is anonymized. Access &
                hosting:
                public release via IEEE DataPort (Standard Dataset; access typically requires an IEEE DataPort
                subscription).
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">#####</td>
              <td>Clarkson University :Multi-modal Longitudinal Children Biometric Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Multi-modal-Longitudinal-Children-Biometric">GitHub
                  <br> Page</a>
              </td>
              <td>
                This longitudinal biometric dataset was collected from six different modalities at approximately
                six-month intervals over a span of 9.5 years at Potsdam Elementary and Potsdam High School. The
                collection, funded by the
                Center for Identification Technology Research (CITeR) and the National Science Foundation, includes:
                Fingerprints,
                Footprints, Face, Ear, NIR Iris, and Voice Recordings. The dataset aims to support research on biometric
                aging in children and
                enhance biometric recognition technologies for younger populations.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">#####</td>
              <td>Clarkson University: Multimodal Longitudinal Infant–Toddler Biometric Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Multi-modal-Longitudinal-Children-Biometric">GitHub
                  <br> Page</a>
              </td>
              <td>
                This longitudinal biometric dataset was collected from six different modalities at approximately 15-day
                to 3-month
                intervals over a span of two years at the Canton Pediatric Office and Clarkson University. The
                collection, funded by
                the Center for Identification Technology Research (CITeR) and the National Science Foundation, includes:
                Fingerprints, Footprints, Face, Ear, NIR Iris, and Voice Recordings. The dataset supports research in
                early-age
                biometric development and recognition performance in infants and toddlers.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">#####</td>
              <td>IBM-UB Online and Offline Multilingual Handwriting Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Dr. Srirangaraj Setlur (UB)</li>
                  <li style="white-space:nowrap;">Email: <a href="mailto:setlur@buffalo.edu">setlur@buffalo.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Clarkson-University-Multimodal-Longitudinal-Infant-Toddler-Biometric-Dataset"
                  target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The IBM-UB dataset is a bilingual and bi-modal handwriting corpus containing both
                <strong>online</strong> (stylus-captured)
                and <strong>offline</strong> (scanned) handwritten documents in multiple languages. It includes
                free-form text, structured forms,
                isolated words, and symbols. This dataset supports multilingual OCR and information retrieval research.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>UB RidgeBase Fingerprint Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">University at Buffalo (UB)</li>
                  <li style="white-space:nowrap;">Contact: <a
                      href="cubs-ridgebase@buffalo.edu">cubs-ridgebase@buffalo.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a class="has-text-centered"
                  href="https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html" target="_blank"
                  rel="noopener">
                  RidgeBase <br> Dataset <br> Page
                </a>

              </td>
              <td>
                RidgeBase is a large-scale benchmark dataset for advancing research in <strong>contactless fingerprint
                  recognition</strong>
                using smartphone cameras. It contains over 15,000 contactless and contact-based fingerprint image pairs
                collected from
                88 individuals under diverse conditions. The dataset supports evaluation of multiple matching scenarios:
                <ul style="margin-top:5px; margin-bottom:5px; padding-left:20px;">
                  <li>Task 1: Single Finger Matching</li>
                  <li>Task 2: Four Finger Matching</li>
                  <li>Task 3: Set-Based Matching</li>
                </ul>
                RidgeBase enables benchmarking for both contactless-to-contactless (CL2CL) and contact-to-contactless
                (C2CL)
                verification and identification research.
              </td>

            <tr>
              <td class="has-text-centered">#######</td>
              <td>GestSpoof: Gesture-Based Spatio-Temporal Representation Learning for Robust Fingerprint Presentation
                Attack Detection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">University at Buffalo (UB)</li>
                  <li style="white-space:nowrap;">Contact: <a
                      href="mailto:cubs-gestspoof@buffalo.edu">cubs-gestspoof@buffalo.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://www.buffalo.edu/cubs/research/datasets/gestspoof-dataset.html" target="_blank"
                  rel="noopener">
                  Dataset <br> Page
                </a>
              </td>
              <td>
                The GestSpoof dataset contains both <strong>real</strong> and <strong>spoof</strong> fingerprint samples
                collected from approximately <strong>23 subjects</strong> using three spoof materials
                (<em>bodydouble_alja</em>, <em>ecoflex_alja</em>, and <em>gelatin_bodydouble</em>). Fingers include
                index, middle, ring, and little, each captured under five motion gestures (<em>Vertical</em>,
                <em>Horizontal</em>, <em>Diag1</em>, <em>Diag2</em>, and <em>Twist</em>). This dataset enables
                spatio-temporal modeling for motion-based fingerprint presentation attack detection by capturing
                differences in elastic deformation between authentic and synthetic fingerprints.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>GestSpoof: Gesture-Based Spatio-Temporal Representation Learning for Robust Fingerprint Presentation
                Attack Detection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">University at Buffalo (UB)</li>
                  <li style="white-space:nowrap;">Contact: <a
                      href="mailto:cubs-gestspoof@buffalo.edu">cubs-gestspoof@buffalo.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://www.buffalo.edu/cubs/research/datasets/gestspoof-dataset.html" target="_blank"
                  rel="noopener">
                  Dataset <br> Page
                </a>
              </td>
              <td>
                The GestSpoof dataset contains both <strong>real</strong> and <strong>spoof</strong> fingerprint samples
                collected from approximately <strong>23 subjects</strong> using three spoof materials
                (<em>bodydouble_alja</em>, <em>ecoflex_alja</em>, and <em>gelatin_bodydouble</em>). Fingers include
                index, middle, ring, and little, each captured under five motion gestures (<em>Vertical</em>,
                <em>Horizontal</em>, <em>Diag1</em>, <em>Diag2</em>, and <em>Twist</em>). This dataset enables
                spatio-temporal modeling for motion-based fingerprint presentation attack detection by capturing
                differences in elastic deformation between authentic and synthetic fingerprints.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Multimodal Biometric Dataset Collection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:WVUBiometricData@mail.wvu.edu">WVUBiometricData@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Multimodal-Biometric-Collection" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The Multimodal Biometric Dataset Collection is a multisite database developed by <strong>West Virginia
                  University</strong> and <strong>Clarkson University</strong> for research under the <strong>CITeR
                  Database Release Agreement</strong>. It includes <strong>iris, face, face video, voice, fingerprint,
                  hand geometry</strong>, and <strong>palmprint</strong> data for over 500 subjects collected across
                multiple visits. The dataset supports multimodal biometric fusion and longitudinal analysis, and
                includes releases: <em>WVU Release 1</em>, <em>WVU Release 2</em>, and <em>CU Release 2</em>.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Quality—Face/Iris Research Ensemble (Q-FIRE)</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Funded by: DHS S&amp;T Directorate and NSF</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Quality-Face-Iris-Research-Ensemble" target="_blank"
                  rel="noopener">
                  GitHub <br>Page
                </a>
              </td>
              <td>
                The Q-FIRE dataset, developed at <strong>Clarkson University</strong>, contains <strong>iris</strong>
                and <strong>face</strong> image sequences collected at varying distances and quality levels. It was
                funded by the <strong>Department of Homeland Security (DHS) Science and Technology Directorate</strong>
                in cooperation with the <strong>National Science Foundation (NSF)</strong>. The dataset supports
                research on multi-biometric fusion and image quality assessment, and was designed for use in <a
                  href="http://iris.nist.gov/irexII/" target="_blank" rel="noopener">IREX II</a> evaluations.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>Clarkson University: Face–Iris Research Ensemble (Q-FIRE II)</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Technical Contact: Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset_Face-Iris-Research-Ensemble-Q-FIRE-II"
                  target="_blank" rel="noopener">
                  GitHub Page
                </a>
              </td>
              <td>
                The <strong>Face–Iris Research Ensemble (Q-FIRE II)</strong> dataset, collected at <strong>Clarkson
                  University</strong>, contains <strong>visible, NIR, and LWIR image sequences</strong> of faces and
                irises at varying distances and qualities, along with <strong>voice recordings</strong>. Funded by the
                <em>Center for Identification Technology Research (CITeR)</em> and the <em>National Science
                  Foundation</em>, this multimodal dataset supports research in <strong>multi-spectral and
                  multi-biometric fusion</strong>, <strong>quality assessment</strong>, and <strong>human identification
                  under diverse conditions</strong>.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>Keystroke Dataset Collection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Keystroke-Collection" target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The Keystroke Dataset was collected at <strong>Clarkson University</strong> from <strong>39
                  participants</strong> across two laboratory sessions, each containing approximately 10K keystrokes.
                The dataset includes <strong>password entries</strong>, <strong>free text responses</strong>, and
                <strong>transcription tasks</strong>, with synchronized video recordings of participants’ facial
                expressions and hand movements. It supports research in <strong>behavioral biometrics</strong> and
                <strong>keystroke dynamics</strong> under controlled experimental settings.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Facial Makeup Datasets</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Michigan State University (MSU)</li>
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Datasets-Facial-Makeup" target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The Facial Makeup Datasets comprise four collections designed to study the effects of <strong>cosmetic
                  makeup</strong> on face recognition performance: <em>YMU</em> (YouTube Makeup), <em>VMU</em> (Virtual
                Makeup), <em>MIW</em> (Makeup in the Wild), and <em>MIFS</em> (Makeup Induced Face Spoofing). These
                datasets include both real and synthetically modified female face images sourced from online media and
                controlled repositories, supporting research in <strong>makeup-robust face recognition</strong> and
                <strong>presentation attack detection</strong>.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#######</td>
              <td>University at Buffalo Dataset for Keystroke Dynamics and Mouse Movements</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">University at Buffalo (UB)</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:cubsinfo@buffalo.edu">cubsinfo@buffalo.edu
                    </a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-for-Keystroke-Dynamics-and-Mouse-Movements"
                  target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The dataset from <strong>University at Buffalo</strong> includes <strong>keystroke dynamics</strong> and
                <strong>mouse movement</strong> data collected under transcription and free-text typing conditions. It
                also captures variations from different keyboard types across sessions. Mouse coordinate and event data
                complement the typing data to support research in <strong>behavioral biometrics</strong> and
                <strong>user interaction analysis</strong>. More details are available at <a
                  href="https://www.buffalo.edu/cubs/research/datasets.html#keystroke" target="_blank"
                  rel="noopener">buffalo.edu/cubs</a>.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#######</td>
              <td>WVU: Iris Biometric In Difficult Conditions Dataset (IBIDC)</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Contact: <a
                      href="mailto:WVUBiometricData@mail.wvu.edu">WVUBiometricData@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Iris-Biometric-In-Difficult-Conditions"
                  target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Iris Biometric In Difficult Conditions (IBIDC)</strong> dataset, developed by <strong>West
                  Virginia University</strong>, contains <strong>off-axis and angled iris images</strong> captured using
                two cameras — a Sony Cyber Shot DSC-F717 and a monochrome infrared camera. The Sony camera operated in
                infrared “night vision” mode but retained RGB sensor data, giving the images a green hue. The dataset
                supports research in <strong>iris recognition under challenging capture conditions</strong>.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Michigan State University: Mobile Face Spoofing Dataset (MFSD)</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Michigan State University (MSU)</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://biometrics.cse.msu.edu/Publications/Databases/MSU_LFW+_back/" target="_blank"
                  rel="noopener">
                  Dataset <br> Page
                </a>
              </td>
              <td>
                The <strong>Mobile Face Spoofing Dataset (MFSD)</strong> was developed at <strong>Michigan State
                  University</strong> to simulate <strong>spoof attacks on smartphones</strong> using cameras that
                replicate input received by face-unlock systems like Android’s Trusted Face. It contains <strong>280
                  video clips</strong> of <strong>photo and video attack attempts</strong> targeting 35 clients. The
                dataset supports research in <strong>presentation attack detection</strong> and <strong>mobile face
                  authentication security</strong>.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>Michigan State University: Tattoo Sketch and Image Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Michigan State University (MSU)</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Tattoo-Sketch-and-Image" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Tattoo Sketch and Image Dataset</strong>, developed at <strong>Michigan State
                  University</strong>, supports research in <strong>tattoo recognition</strong> and <strong>cross-domain
                  matching</strong> between hand-drawn sketches and photographic tattoo images. It enables evaluation of
                sketch-based retrieval and forensic tattoo identification techniques. More details are available at <a
                  href="http://biometrics.cse.msu.edu/pubs/databases.html" target="_blank"
                  rel="noopener">biometrics.cse.msu.edu</a>.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Clarkson University: Nail-to-Nail Prototype Finger Photo Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Funded by: CITeR</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Nail-to-Nail-Prototype-Finger-Photo" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Nail-to-Nail (N2N) Prototype Finger Photo Dataset</strong>, developed at <strong>Clarkson
                  University</strong> and funded by <strong>CITeR</strong>, includes 15 photographs acquired around the
                finger to support <strong>nail-to-nail fingerprint analysis</strong>. It was released through
                <strong>NIST Special Dataset 302</strong> as part of the N2N Fingerprint Challenge. Researchers can
                request access directly from <a
                  href="https://www.nist.gov/itl/iad/image-group/nist-special-database-302" target="_blank"
                  rel="noopener">NIST</a>. Additional details are available in the <a
                  href="https://github.com/CITeR-Research/Dataset-Nail-to-Nail-Prototype-Finger-Photo" target="_blank"
                  rel="noopener">GitHub repository</a>.
              </td>
            </tr>

            <tr>
              <td class="has-text-centered">######</td>
              <td>Clarkson University: Liveness Detection Competition Fingerprint and Iris Datasets (LivDet)</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Funded by: CITeR</li>
                  <li style="white-space:nowrap;">Contact: <a href="mailto:citer@clarkson.edu">citer@clarkson.edu</a>
                  </li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Datasets-Liveness-Detection-Competition-Fingerprint-and-Iris"
                  target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Liveness Detection Competition (LivDet)</strong> datasets, developed at <strong>Clarkson
                  University</strong> and funded by <strong>CITeR</strong>, include fingerprint and iris images
                collected across multiple years (2009, 2011, 2013, 2015, 2017) featuring both <strong>live and spoof
                  samples</strong> made from materials such as gelatin, Play-Doh, Ecoflex, paper, and patterned contact
                lenses. These datasets support research in <strong>presentation attack detection</strong> and
                <strong>biometric liveness verification</strong>. Additional information is available at <a
                  href="http://livdet.org/" target="_blank" rel="noopener">livdet.org</a>, and related repositories for
                newer releases (2020, 2021, 2024) can be found in the same GitHub organization.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>WVU: Synthetic Iris Dataset Collection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Contact: <a
                      href="mailto:WVUBiometricData@mail.wvu.edu">WVUBiometricData@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Synthetic-Iris-Collection" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Synthetic Iris Dataset Collection</strong>, developed at <strong>West Virginia
                  University</strong>, contains both <strong>texture-based</strong> and <strong>model-based</strong>
                synthetic iris images. The texture-based set includes 1,000 classes with seven samples per class, while
                the model-based set includes 5,000 subjects with left and right eyes, each having 16 images. These
                datasets support research on <strong>iris synthesis</strong>, <strong>biometric simulation</strong>, and
                <strong>template generation</strong>. More details are available at <a
                  href="https://biic.wvu.edu/data-sets/synthetic-iris-dataset" target="_blank"
                  rel="noopener">biic.wvu.edu</a>.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#######</td>
              <td>WVU: Multispectral Ocular Biometrics Collection</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Contact: <a
                      href="mailto:Jeremy.dawson@mail.wvu.edu">Jeremy.dawson@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Multispectral-Ocular-Biometrics-Collection" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Multispectral Ocular Biometrics Collection</strong>, developed at <strong>West Virginia
                  University</strong>, supports research in <strong>ocular recognition</strong> using multispectral
                imaging. The dataset captures <strong>NIR</strong> and <strong>RGB</strong> images of the eye to analyze
                <strong>sclera texture</strong>, <strong>vasculature patterns</strong>, and <strong>periocular
                  cues</strong>. It enables studies on enhancing <strong>non-frontal iris recognition</strong> and
                integrating multiple spectral channels for improved biometric performance.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">#######</td>
              <td>Clarkson University: Experimental Quality Latent Fingerprint Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Technical Contact: Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Experimental-Quality-Latent-Fingerprint"
                  target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Experimental Quality Latent Fingerprint Dataset</strong>, developed at <strong>Clarkson
                  University</strong> in collaboration with <strong>SUNY-Canton</strong>, contains latent fingerprints
                captured using multiple devices, lighting conditions, surfaces, and placements. The dataset includes
                direct surface images, lift images, and lift scans. It was funded by the <strong>DHS Science and
                  Technology Directorate</strong> and the <strong>National Science Foundation</strong>, and is available
                to CITeR members under the Database Release Agreement.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>Clarkson University: Liveness Detection Competition Iris Dataset 2020 (LivDet)</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Technical Contact: Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-Liveness-Detection-Competition-Iris-2020"
                  target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Liveness Detection Competition Iris Dataset 2020 (LivDet)</strong>, developed at
                <strong>Clarkson University</strong>, contains <strong>iris images from live and spoof samples</strong>
                created using materials such as paper and patterned contact lenses. This dataset supports research in
                <strong>iris presentation attack detection</strong> and <strong>biometric liveness
                  verification</strong>. It is available to CITeR members under the Database Release Agreement.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>Clarkson University: Liveness Detection NonContact Fingerprint Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Technical Contact: Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Liveness-Detection-NonContact-Fingerprint" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Liveness Detection NonContact Fingerprint Dataset</strong>, developed at <strong>Clarkson
                  University</strong>, contains <strong>4-finger photo images</strong> from <strong>live and spoof
                  samples</strong> created using <em>ecoflex</em>, <em>gelatin</em>, and <em>Play-Doh</em>. This dataset
                supports research in <strong>noncontact fingerprint presentation attack detection</strong> and
                <strong>spoof-resilient biometric systems</strong>.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>Clarkson University: Liveness Detection Competition 2021 Face (LivDet)</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Technical Contact: Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Datasets-Liveness-Detection-Competition-2021-Face"
                  target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Liveness Detection Competition 2021 Face (LivDet)</strong> dataset, developed at
                <strong>Clarkson University</strong>, contains <strong>live and spoof face images</strong> captured
                using DSLR, iPhone X, Samsung Galaxy S9, Google Pixel, and Basler sensors. It includes nine types of
                presentation attack instruments (PAIs) such as <em>printed paper displays</em>, <em>laptop screen
                  replays</em>, <em>2D photo masks</em>, <em>3D printed masks</em> (low, medium, high quality),
                <em>wearable silicone masks</em>, and <em>video display attacks</em>. The dataset supports research in
                <strong>face presentation attack detection</strong> under diverse capture conditions.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>Clarkson University: Liveness Detection Competition 2024 Face (LivDet)</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">Clarkson University (CU)</li>
                  <li style="white-space:nowrap;">Technical Contact: Dr. Stephanie Schuckers</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:sschucke@clarkson.edu">sschucke@clarkson.edu</a></li>
                  <li style="white-space:nowrap;">Dataset Access: <a
                      href="mailto:citer@clarkson.edu">citer@clarkson.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Datasets-Liveness-Detection-Competition-2024-Face"
                  target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Liveness Detection Competition 2024 Face (LivDet)</strong> dataset, developed at
                <strong>Clarkson University</strong>, contains <strong>live and spoof face images</strong> representing
                nine <em>presentation attack instrument (PAI)</em> categories. It includes <em>bobblehead models</em>,
                <em>projection attacks</em> (2D and 3D), <em>half-cloth masks</em>, <em>print and replay attacks</em>,
                <em>silicon masks</em>, and <em>3D masks</em> of varying quality. Collected using nine sensors under
                diverse lighting and device conditions, this dataset supports research on <strong>robust face
                  presentation attack detection</strong> and <strong>cross-sensor generalization</strong>.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>WVU: Contactless Fingerprint Collection I</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Contact: Dr. Jeremy Dawson</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:jeremy.dawson@mail.wvu.edu">jeremy.dawson@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Contactless-Fingerprint-Collection-I" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Contactless Fingerprint Collection I</strong> dataset from <strong>West Virginia
                  University</strong> includes data from <strong>216 participants</strong>, featuring <em>hand
                  photos</em> captured with a Canon 5DSR, <em>contactless fingerprint scans</em> from Gemalto/Thales and
                Morpho Wave devices, <em>contact-based prints</em> from a Crossmatch Guardian sensor, and <em>mobile
                  captures</em> using an iPhone X. The dataset supports research on <strong>cross-sensor
                  interoperability</strong> and <strong>contactless fingerprint biometrics</strong>.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>WVU: Contactless Fingerprint Collection II</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Contact: Dr. Jeremy Dawson</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:jeremy.dawson@mail.wvu.edu">jeremy.dawson@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Contactless-Fingerprint-Collection-II" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>Contactless Fingerprint Collection II</strong> dataset from <strong>West Virginia
                  University</strong> includes data from <strong>500 participants</strong> captured using multiple
                contactless and contact-based systems. It features <em>Guardian and Kojak contact prints</em>,
                <em>Morpho Wave</em> and <em>Gemalto contactless sensors</em>, <em>DSLR hand geometry photos</em>, and
                <em>smartphone captures</em> (Samsung S20/S21) under both controlled and operational conditions across
                multiple sessions. This dataset enables research in <strong>cross-sensor evaluation</strong> and
                <strong>contactless fingerprint interoperability</strong>.
              </td>
            </tr>


            <tr>
              <td class="has-text-centered">######</td>
              <td>WVU: ARDEC Multimodal Using Fieldable Devices</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">West Virginia University (WVU)</li>
                  <li style="white-space:nowrap;">Contact: Dr. Jeremy Dawson</li>
                  <li style="white-space:nowrap;">Email: <a
                      href="mailto:jeremy.dawson@mail.wvu.edu">jeremy.dawson@mail.wvu.edu</a></li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/ARDEC-Multimodel-Using-Fieldable-Devices" target="_blank"
                  rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>ARDEC Multimodal Using Fieldable Devices</strong> dataset from <strong>West Virginia
                  University</strong> includes data from <strong>300 participants</strong> collected using both
                <em>standard biometric sensors</em> and <em>portable multi-biometric devices</em>. It features <em>iris
                  images</em> from iCam TD 100, <em>five-pose face photos</em> (Canon 5DSR), and <em>slap / rolled
                  fingerprints</em> (Crossmatch Guardian). Additional data were acquired using <em>Northrop Grumman’s
                  multi-biometric platform</em> (Galaxy S5 + Kojak) and the <em>Crossmatch SEEK Avenger</em>, capturing
                faces, irises, and fingerprints under operational, uncontrolled conditions. This dataset supports
                research in <strong>portable biometric systems</strong> and <strong>cross-platform evaluation</strong>.
              </td>
            </tr>



            <tr>
              <td class="has-text-centered">######</td>
              <td>LibriVOC Dataset</td>
              <td>
                <ul style="margin:0; padding-left:1.2em; list-style:none;">
                  <li style="white-space:nowrap;">University at Buffalo (UB)</li>
                  <li style="white-space:nowrap;">PIs: Dr. Siwei Lyu, Dr. David Doermann, Dr. Srirangaraj Setlur</li>
                  <li style="white-space:nowrap;">CITeR Project ID: 22S-01B</li>
                </ul>
              </td>
              <td class="has-text-centered">
                <a href="https://github.com/CITeR-Research/Dataset-LibrVOC" target="_blank" rel="noopener">
                  GitHub <br> Page
                </a>
              </td>
              <td>
                The <strong>LibriVOC Dataset</strong> is a large-scale, open-source corpus developed for <strong>vocoder
                  artifact detection</strong>. Derived from <em>LibriTTS</em> and <em>LibriSpeech</em> datasets,
                LibriVOC provides benchmark audio samples sourced from <em>LibriVox audiobooks</em> to support research
                in <strong>neural vocoder identification</strong> and <strong>speech synthesis forensics</strong>. More
                details are available at <a href="https://github.com/csun22/LibriVoc-Dataset" target="_blank"
                  rel="noopener">LibriVoc GitHub</a>.
              </td>
            </tr>






          </tbody>
        </table>
      </div>
    </div>
  </section>





  <!-- Video Display Section -->
  <section class="section" style="display:flex; flex-direction:column; align-items:center; padding:40px 0;">
    <h2 class="title is-3 has-text-centered" style="margin-bottom:24px;">CITeR Activities</h2>

    <div style="
      width:1280px;
      height:720px;
      overflow:hidden;
      border-radius:12px;
      box-shadow:0 0 12px rgba(0,0,0,0.25);
      position:relative;
      background:#000;
    ">
      <video id="original-frame-video" autoplay muted loop playsinline preload="metadata" style="
             position:absolute;
             top:50%;
             left:50%;
             transform:translate(-50%, -50%);
             min-width:100%;
             min-height:100%;
             object-fit:cover;
           ">
        <source src="https://citer.clarkson.edu/wp-content/uploads/2019/08/Biometrics.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
  </section>





  <section class="section" style="background-color:#fafafa; padding:40px 0;">
    <h2 class="title is-3 has-text-centered" style="margin-bottom:24px;">Affiliates</h2>
    <div class="container" style="max-width:1500px; margin:0 auto; text-align:center;">
      <div style="
      display:flex;
      justify-content:space-around;
      align-items:center;
      flex-wrap:wrap;
      gap:30px;
    ">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/stacked-grn-logo.png" alt="Clarkson University"
          style="height:200px; object-fit:contain;">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/west-virginia.png"
          alt="West Virginia University" style="height:200px; object-fit:contain;">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/buffalo.png" alt="University at Buffalo"
          style="height:200px; object-fit:contain;">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2019/08/michigan.png" alt="Michigan State University"
          style="height:200px; object-fit:contain;">
        <img src="https://citer.clarkson.edu/wp-content/uploads/2020/01/idiap-logo.png" alt="Idiap Research Institute"
          style="height:200px; object-fit:contain;">
        <img
          src="https://citer.clarkson.edu/wp-content/uploads/2025/04/8716-01-Charlotte-Master-File-v7_1-1-1024x1024.png"
          alt="UNC Charlotte" style="height:200px; object-fit:contain;">
      </div>
    </div>
  </section>






  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/redwankarimsony" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>